import re
import random
import math
from collections import defaultdict


class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a language model
    from a given text.
    It supports language generation and the evaluation of a given string.
    The class can be applied on both word level and character level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.

            Args:
                n      (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
                chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                              Defaults to False
        """
        self.n = n
        self.chars = chars
        # a dictionary of the form {ngram:count}, holding counts of all n-grams in the specified text.
        self.model_dict = defaultdict(int)

    def build_model(self, text):
        """populates the instance variable model_dict.

            Args:
                text (str): the text to construct the model from.
        """
        n_grams = self.__get_n_grams(text)
        for ngram in n_grams:
            if self.chars and len(ngram) == self.n:
                self.model_dict[ngram] += 1
            elif not self.chars and len(ngram.split(' ')) == self.n:
                self.model_dict[ngram] += 1

    def get_model_dictionary(self):
        """Return the dictionary class object

            Return:
                dict.   a dictionary of the form {ngram:count}.
        """
        return self.model_dict

    def get_model_window_size(self):
        """Return the size of the context window (the n in "n-gram")

            Return:
                int.    the size of the context window.
        """
        return self.n

    def generate(self, context=None, n=20):
        """Return a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context sampled
        from the models' contexts distribution. The Generation is stopped before the n'th word if the
        contexts are exhausted (if the model dictionary does not contain the context).
        If the length of the specified context exceeds (or equal to)
        the specified n, the method return the prefix of length n of the specified context.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n       (int): the length of the string to be generated.

            Return:
                str:    the generated text.

        """
        if self.chars:
            return self.__generate_new_string_by_chars(context, n)
        else:
            return self.__generate_new_string_by_words(context, n)

    def evaluate(self, text):
        """Returns the log-likelihood of the specified text to be a product of the model.
       Laplace smoothing applied if necessary, i.e if the text contains n-grams that does
       not included in the model dictionary.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float:   the float should reflect the (log) probability.
        """
        smooth = False
        n_grams = self.__get_n_grams(text)
        log_le = 0

        for n_gram in n_grams:
            if not self.__ngram_existence(n_gram):
                smooth = True
                break

        for n_gram in n_grams:
            prob = self.smooth(n_gram) if smooth else self.__n_gram_prob(n_gram)
            if prob > 0:
                log_le += math.log(prob)

        return log_le

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float:  the smoothed probability.
        """
        pre_n_gram = self.__get_pre_n_gram(ngram)
        numerator = self.__get_n_gram_counter(ngram) + 1
        denominator = self.__get_pre_n_gram_counter(pre_n_gram) + len(self.model_dict.keys())
        return numerator / denominator

    """Private Methods"""

    def __get_n_grams(self, text):
        """Return the n-grams of the given text.

            Args:
                text (str): the text to construct the n-grams from.

            Returns:
                list:   the n-grams list.
        """
        if self.chars:
            return self.__get_chars_n_grams(text)
        else:
            return self.__get_words_n_grams(text)

    def __get_chars_n_grams(self, text):
        """Return the n-gram of characters of the given text.

            Args:
                text (str): the text to construct the n-grams of characters from.

            Returns:
                list:   the n-grams list.
        """
        n_grams = [text[:i] for i in range(1, self.n)]
        n_grams.extend([text[i: i + self.n] for i in range(len(text) - self.n + 1)])
        return n_grams

    def __get_words_n_grams(self, text):
        """Return the n-gram of word tokens of the given text.

            Args:
                text (str): the text to construct the n-grams of word tokens from.

            Returns:
                list:   the n-grams list.
        """
        tokens = re.split(' ', text)
        n_grams = [' '.join(tokens[:i]) for i in range(1, self.n)]
        n_grams.extend([' '.join(tokens[i: i + self.n]) for i in range(len(tokens) - self.n + 1)])
        return n_grams

    def __ngram_existence(self, ngram):
        """Returns boolean which indicates if the given ngram exists in the model.

            Args:
                ngram (str): the ngram to check its existence in the model dictionary

            Returns:
                bool:   true for exist or false for non-exist.
        """

        # if ngram in model dictionary return
        if ngram in self.model_dict.keys():
            return True

        # check if the ngram is a prefix of one of the model ngrams - words
        elif not self.chars and len(ngram.split(' ')) < self.n:
            set_short_keys = set()

            for key in self.model_dict.keys():
                result = ' '.join(key.split(' ')[:len(ngram.split(' '))])
                set_short_keys.add(result)

            if ngram in set_short_keys:
                return True

        # check if the ngram is a prefix of one of the model ngrams - chars
        elif self.chars and len(ngram) < self.n:
            set_short_keys = set()

            for key in self.model_dict.keys():
                result = key[:len(ngram)]
                set_short_keys.add(result)

            if ngram in set_short_keys:
                return True

        return False

    def __n_gram_prob(self, ngram):
        """Return the probability of an ngram.

            Args:
                ngram (str). string of the ngram

            Returns:
                float:  the probability of the given ngram
        """
        pre_n_gram = self.__get_pre_n_gram(ngram)

        numerator = self.__get_n_gram_counter(ngram)
        denominator = self.__get_pre_n_gram_counter(pre_n_gram)

        return numerator / denominator

    def __get_pre_n_gram(self, ngram):
        """Return the prefix of the ngram. e.g the prefix of the string 'a cat sat' is 'a cat'.

            Args:
                ngram (str). get the prefix form this ngram.

            Returns:
                str: the prefix of the given ngram

        """
        if self.chars:
            return self.__get_pre_chars_n_gram(ngram)
        else:
            return self.__get_pre_words_n_gram(ngram)

    def __get_pre_chars_n_gram(self, ngram):
        """Return the chars' prefix of the ngram.

            Args:
                ngram (str). get the chars' prefix form this ngram.

            Returns:
                str:    the prefix of the given ngram
        """
        return ngram[:-1]

    def __get_pre_words_n_gram(self, ngram):
        """Return the words' prefix of the ngram.

            Args:
                ngram (str). get the words' prefix form this ngram.

            Returns:
                str:    the prefix of the given ngram
        """
        words = re.split(' ', ngram)
        return ' '.join(words[:-1])

    def __get_n_gram_counter(self, ngram):
        """Return the sum of n-grams which start with the given n-gram .

            Args:
                ngram (str). the ngram to count in the model dictionary

            Returns:
                int:    number of n-grams which start with this ngram.
        """
        if self.chars:
            return self.__get_n_gram_counter_chars(ngram)
        else:
            return self.__get_n_gram_counter_words(ngram)

    def __get_n_gram_counter_chars(self, ngram):
        """Return the sum of char n-grams which start with the given n-gram .

            Args:
                ngram (str). the char ngram to count in the model dictionary

            Returns:
                int:    number of char n-grams which start with this ngram.
        """
        chars = ngram[:len(ngram)]

        # if the length equals to n, get the counter from the model dictionary
        if len(chars) == self.n:
            numerator = 0 if ngram not in self.model_dict.keys() else self.model_dict[ngram]

        # else count how many ngram starts with this ngram
        else:
            numerator = sum([count for key, count in self.model_dict.items() if key[:len(chars)] == chars])

        return numerator

    def __get_n_gram_counter_words(self, ngram):
        """Return the sum of word n-grams which start with the given n-gram .

            Args:
                ngram (str). the word ngram to count in the model dictionary

            Returns:
                int:    number of word n-grams which start with this ngram.
        """
        words = re.split(' ', ngram)[:len(ngram)]

        if len(words) == self.n:
            numerator = 0 if ngram not in self.model_dict.keys() else self.model_dict[ngram]
        else:
            numerator = sum(
                [count for key, count in self.model_dict.items() if re.split(' ', key)[:len(words)] == words])

        return numerator

    def __get_pre_n_gram_counter(self, pre_n_gram):
        """Return the sum of the pre n-gram - count all the keys which start with this prefix.

            Args:
                pre_n_gram (str). the pre n-gram to count in the model dictionary

            Returns:
                int:    number of keys which start with this pre_n_gram.
        """
        if self.chars:
            return self.__get_pre_n_gram_counter_chars(pre_n_gram)
        else:
            return self.__get_pre_n_gram_counter_words(pre_n_gram)

    def __get_pre_n_gram_counter_chars(self, pre_n_gram):
        """Return the sum of the chars pre n-gram - count all the keys which start with this prefix.

            Args:
                pre_n_gram (str). the chars pre n-gram to count in the model dictionary

            Returns:
                int:    number of keys which start with this chars pre_n_gram.
        """
        denominator = sum(
            [count for key, count in self.model_dict.items() if key[:len(pre_n_gram)] == pre_n_gram[:len(pre_n_gram)]])

        return denominator

    def __get_pre_n_gram_counter_words(self, pre_n_gram):
        """Return the sum of the words pre n-gram - count all the keys which start with this prefix.

            Args:
                pre_n_gram (str). the words pre n-gram to count in the model dictionary

            Returns:
                int:    number of keys which start with this words pre_n_gram.
        """

        # if the pre ngram is empty (the ngram is unigram) then don't split the word.
        if len(pre_n_gram) == 0:
            pre_n_gram_tokens = []
        else:
            pre_n_gram_tokens = pre_n_gram.split(' ')

        denominator = sum(
            [count for key, count in self.model_dict.items() if
             re.split(' ', key)[:len(pre_n_gram_tokens)] == re.split(' ', pre_n_gram)[:len(pre_n_gram_tokens)]])

        return denominator

    def __generate_new_string_by_chars(self, context, n):
        """Generate a new string of chars based on the given context with number of chars equal to n.
        If no context is specified the context sampled from the models' contexts distribution.
        The Generation is stopped before the n'th word if the contexts are exhausted
        (if the model dictionary does not contain the context). If the length of the specified context
        exceeds (or equal to) the specified n, the method return the prefix of length n of the specified context.

            Args:
                context (str):  generate a new string based on this context
                n       (int):  the length of the generated string

            Returns:
                str:    the new generated string of length n based on the context.

        """
        generated_str = ''

        # if the context longer than n - cut the context till the n char
        if context is not None and len(context) >= n:
            return context[:n]

        # if the context is no longer than n - copy the context to the beginning of the new string
        if context is not None and len(context) != 0:
            generated_str += context

        # while the new string length is less than n - generate new char based on the previous context.
        # if the context does not exist in the model - stop (when the next char equals to None).
        while len(generated_str) < n:
            next_char = self.__get_next_char(generated_str)
            if next_char is not None:
                generated_str += next_char
            else:
                break

        return generated_str

    def __generate_new_string_by_words(self, context, n):
        """Generate a new string of words based on the given context with number of words equal to n.
        If no context is specified the context sampled from the models' contexts distribution.
        The Generation is stopped before the n'th word if the contexts are exhausted
        (if the model dictionary does not contain the context). If the length of the specified context
        exceeds (or equal to) the specified n, the method return the prefix of length n of the specified context.

            Args:
                context (str):  generate a new string based on this context
                n       (int):  the length of the generated string

            Returns:
                str:    the new generated string of length n based on the context.

        """
        generated_str = []

        # if the context longer than n - cut the context till the n word
        if context is not None and len(re.split(' ', context)) >= n:
            return ' '.join(re.split(' ')[:n])

        # if the context is no longer than n - copy the context to the beginning of the new string
        if context is not None and len(context) != 0:
            generated_str.extend(re.split(' ', context))

        # while the new string length is less than n - generate new word based on the previous context.
        # if the context does not exist in the model - stop (when the next word equals to None).
        while len(generated_str) < n:
            next_word = self.__get_next_word(generated_str)
            if next_word is not None:
                generated_str.append(next_word)
            else:
                break

        return ' '.join(generated_str)

    def __get_next_char(self, context):
        """Return the next char based on the distribution of the context.
                   If there is no next char, return None.

            Args:
                context (str): the context in which we sample the next char from its distribution

            Returns:
                char or None: char of the next char or None if can not generate more chars.

        """
        counter_dict = defaultdict(int)

        # get the first chars from all keys if the context length is zero. (unigrams)
        if context is None or len(context) == 0:
            for key, value in self.model_dict.items():
                key = key[0]
                counter_dict[key] += value

        else:
            len_context = len(context)

            # if the context length less than self.n, check if the model has keys
            # which starts with this context, if yes count it as +1 for this key.
            if len_context < self.n:
                for key, value in self.model_dict.items():
                    arr_key = key
                    if arr_key[:len_context] == context:
                        counter_dict[arr_key[len_context]] += value

            # if the context length longer or equal than self.n, check if the model has keys
            # which equal to the last self.n chars in the context, if yes count it as +1 for this key.
            else:
                for key, value in self.model_dict.items():
                    arr_key = key
                    if arr_key[:self.n - 1] == context[-self.n + 1:] or self.n == 1:
                        counter_dict[arr_key[self.n - 1]] += value

        # calculate the next possible char weights
        next_options = list(counter_dict.keys())
        weights = [weight / sum(counter_dict.values()) for weight in counter_dict.values()]

        # sample from distribution the next char if there are next possible chars
        if len(next_options) > 0:
            return random.choices(next_options, weights, k=1)[0]
        else:
            return None

    def __get_next_word(self, context):
        """Return the next word based on the distribution of the context.
        If there is no next word, return None.

            Args:
                context (str): the context in which we sample the next word from its distribution

            Returns:
                str or None: string of the next word or None if can not generate more words.

        """
        counter_dict = defaultdict(int)

        # get the first words from all keys if the context length is zero. (unigrams)
        if context is None or len(context) == 0:
            for key, value in self.model_dict.items():
                key = re.split(' ', key)[0]
                counter_dict[key] += value

        else:
            len_context = len(context)

            # if the context length less than self.n, check if the model has keys
            # which starts with this context, if yes count it as +1 for this key.
            if len_context < self.n:
                for key, value in self.model_dict.items():
                    arr_key = re.split(' ', key)
                    if arr_key[:len_context] == context:
                        counter_dict[arr_key[len_context]] += value

            # if the context length longer or equal than self.n, check if the model has keys
            # which equal to the last self.n chars in the context, if yes count it as +1 for this key.
            else:
                for key, value in self.model_dict.items():
                    arr_key = re.split(' ', key)
                    if arr_key[:self.n - 1] == context[-self.n + 1:] or self.n == 1:
                        counter_dict[arr_key[self.n - 1]] += value

        # calculate the next possible char weights
        next_options = list(counter_dict.keys())
        weights = [weight / sum(counter_dict.values()) for weight in counter_dict.values()]

        # sample from distribution the next char if there are next possible chars
        if len(next_options) > 0:
            return random.choices(next_options, weights, k=1)[0]
        else:
            return None


def normalize_text(text):
    """Returns a normalized version of the specified string.
    The process of normalization is as follows:
    1. lower case -  transform all words into their lower case to map same words (The, the).
    2. pad punctuations with whitespaces - separate between words and punctuations

          Args:
            text (str): the text to normalize

          Returns:
            string: the normalized text.
    """

    if text is not None:
        # transform text to lower case
        norm_text = text.lower()
        # pad all punctuations with whitespaces
        norm_text = re.sub(
            r'([\-_/<>%\"\'`~.,!?(){\}:;&\[\]$^*\\\n\t])', r' \1 ', norm_text)  # padd tokens with spaces
        # remove double whitespaces
        norm_text = re.sub(r'\s{2,}', ' ', norm_text)
        norm_text = norm_text.strip()
        return norm_text

    else:
        return text


def who_am_i():  # this is not a class method
    """Returns a dictionary with your name, id number and email. keys=['name', 'id','email']
    Make sure you return your own info!
    """
    return {'name': 'Amit Damri', 'id': '312199698', 'email': 'amitdamr@post.bgu.ac.il'}
